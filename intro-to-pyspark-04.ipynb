{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Introduction to Spark In-memmory Computing via Python PySpark </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spark is an implementation of the MapReduce programming paradigm that operates on in-memory data and allows data reuses across multiple computations.\n",
    "- Performance of Spark is significantly better than its predecessor, Hadoop MapReduce. \n",
    "- Spark's primary data abstraction is Resilient Distributed Dataset (RDD):\n",
    "    - Read-only, partitioned collection of records\n",
    "    - Created (aka written) through deterministic operations on data:\n",
    "        - Loading from stable storage\n",
    "        - Transforming from other RDDs\n",
    "        - Generating through coarse-grained operations such as map, join, filter ...\n",
    "    - Do not need to be materialized at all time and are recoverable via **data lineage**\n",
    "\n",
    "<img src=\"figures/spark2_arch.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently Loaded Modulefiles:\r\n",
      "  1) anaconda3/4.2.0   2) matlab/2015a      3) zeromq/4.1.5\r\n"
     ]
    }
   ],
   "source": [
    "!module list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "#!/usr/bin/env bash\r\n",
      "\r\n",
      "# This file is sourced when running various Spark programs.\r\n",
      "# Copy it as spark-env.sh and edit that to configure Spark for your site.\r\n",
      "\r\n",
      "# Options read in YARN client mode\r\n",
      "#SPARK_EXECUTOR_INSTANCES=\"2\" #Number of workers to start (Default: 2)\r\n",
      "#SPARK_EXECUTOR_CORES=\"1\" #Number of cores for the workers (Default: 1).\r\n",
      "#SPARK_EXECUTOR_MEMORY=\"1G\" #Memory per Worker (e.g. 1000M, 2G) (Default: 1G)\r\n",
      "#SPARK_DRIVER_MEMORY=\"512M\" #Memory for Master (e.g. 1000M, 2G) (Default: 512 Mb)\r\n",
      "#SPARK_YARN_APP_NAME=\"spark\" #The name of your application (Default: Spark)\r\n",
      "#SPARK_YARN_QUEUE=\"default\" #The hadoop queue to use for allocation requests (Default: default)\r\n",
      "#SPARK_YARN_DIST_FILES=\"\" #Comma separated list of files to be distributed with the job.\r\n",
      "#SPARK_YARN_DIST_ARCHIVES=\"\" #Comma separated list of archives to be distributed with the job.\r\n",
      "\r\n",
      "# Generic options for the daemons used in the standalone deploy mode\r\n",
      "\r\n",
      "# Alternate conf dir. (Default: ${SPARK_HOME}/conf)\r\n",
      "export SPARK_CONF_DIR=${SPARK_CONF_DIR:-/usr/hdp/current/spark2-historyserver/conf}\r\n",
      "\r\n",
      "# Where log files are stored.(Default:${SPARK_HOME}/logs)\r\n",
      "#export SPARK_LOG_DIR=${SPARK_HOME:-/usr/hdp/current/spark2-historyserver}/logs\r\n",
      "export SPARK_LOG_DIR=/var/log/spark2\r\n",
      "\r\n",
      "# Where the pid file is stored. (Default: /tmp)\r\n",
      "export SPARK_PID_DIR=/var/run/spark2\r\n",
      "\r\n",
      "#Memory for Master, Worker and history server (default: 1024MB)\r\n",
      "export SPARK_DAEMON_MEMORY=1024m\r\n",
      "\r\n",
      "# A string representing this instance of spark.(Default: $USER)\r\n",
      "SPARK_IDENT_STRING=$USER\r\n",
      "\r\n",
      "# The scheduling priority for daemons. (Default: 0)\r\n",
      "SPARK_NICENESS=0\r\n",
      "\r\n",
      "export HADOOP_HOME=${HADOOP_HOME:-/usr/hdp/current/hadoop-client}\r\n",
      "export HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-/usr/hdp/current/hadoop-client/conf}\r\n",
      "\r\n",
      "# The java implementation to use.\r\n",
      "#export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.141-2.b16.el7_4.x86_64/jre\r\n",
      "export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.141-1.b16.el7_3.x86_64/jre\r\n"
     ]
    }
   ],
   "source": [
    "!cat /etc/hadoop/synced_conf/spark2/spark-env.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting Started\n",
    "\n",
    "Spark stores data in memory. This memory space is represented by variable **sc** (SparkContext). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, '/usr/hdp/2.6.0.3-8/spark2/python')\n",
    "sys.path.insert(0, '/usr/hdp/2.6.0.3-8/spark2/python/lib/py4j-0.10.4-src.zip')\n",
    "\n",
    "os.environ['SPARK_HOME'] = '/usr/hdp/2.6.0.3-8/spark2/'\n",
    "os.environ['SPARK_CONF_DIR'] = '/etc/hadoop/synced_conf/spark2/'\n",
    "os.environ['PYSPARK_PYTHON'] = '/software/anaconda3/4.2.0/bin/python'\n",
    "\n",
    "import pyspark\n",
    "conf = pyspark.SparkConf()\n",
    "conf.setMaster(\"yarn\")\n",
    "conf.set(\"spark.driver.memory\",\"4g\")\n",
    "conf.set(\"spark.executor.memory\",\"60g\")\n",
    "conf.set(\"spark.num.executors\",\"3\")\n",
    "conf.set(\"spark.executor.cores\",\"12\")\n",
    "\n",
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x2b8643fc8dd8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textFile = sc.textFile(\"/repository/complete-shakespeare.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/repository/complete-shakespeare.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0\n"
     ]
    }
   ],
   "source": [
    "print (textFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What does Spark do with my data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Storage Level:**\n",
    "- Does RDD use disk?\n",
    "- Does RDD use memory?\n",
    "- Does RDD use off-heap memory?\n",
    "- Should an RDD be serialized (while persisting)?\n",
    "- How many replicas (default: 1) to use (can only be less than 40)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(False, False, False, False, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.getStorageLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/repository/complete-shakespeare.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(False, True, False, False, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.getStorageLevel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By default, each transformed RDD may be recomputed each time you run an action on it.\n",
    "- It is also possible to *persist* RDD in memory using *persist()* or *cache()*\n",
    "    - *persist()* allows you to specify level of storage for RDD\n",
    "    - *cache()* only persists RDD in memory\n",
    "    - To retire RDD from memory, *unpersist()* is called"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. WordCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data operations in Spark are categorized into two groups, *transformation* and *action*. \n",
    "- A *transformation* creates new dataset from existing data. Examples of *transformation* include map, filter, reduceByKey, and sort. \n",
    "- An *action* returns a value to the driver program (aka memory space of this notebook) after running a computation on the data set. Examples of *action* include count, collect, reduce, and save. \n",
    "\n",
    "\"All transformations in Spark are lazy, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset (e.g. a file). The transformations are only computed when an action requires a result to be returned to the driver program.\" -- Spark Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDD Operations in Spark\n",
    "\n",
    "**Transformations: **\n",
    "\n",
    "- *map*(f: T -> U) : RDD[T] -> RDD[U]\n",
    "- *filter*(f: T -> Bool) : RDD[T] -> RDD[T]\n",
    "- *flatMap*(f: T -> Seq[U]) : RDD[T] -> RDD[U]\n",
    "- *sample*(*fraction*: Float) : RDD[T] -> RDD[T] (deterministic sampling)\n",
    "- *groupByKey*() : RDD[(K,V)] -> RDD[(K, Seq[V])]\n",
    "- *reduceByKey*(f: (V,V) -> V) : RDD[(K,V)] -> RDD[(K,V)]\n",
    "- *union*() : (RDD[T], RDD[T]) -> RDD[T]\n",
    "- *join*() : (RDD[(K,V)], RDD[(K,W)]) -> RDD[(K,(V,W))]\n",
    "- *cogroup*() : (RDD[(K,V)], RDD[(K,W)] -> RDD[(K, (Seq[V],Seq[W]))]\n",
    "- *crossProduct*() : (RDD[T], RDD[U]) -> RDD[(T,U)]\n",
    "- *mapValues*(f: V -> W) : RDD[(K,V)] -> RDD[(K,W)] (preserves partitioning)\n",
    "- *sort*(c: Comparator[K]) :  RDD[(K,V)] -> RDD[(K,V)]\n",
    "- *partitionBy*(p: Partitioner[K]) : RDD[(K,V)] -> RDD[(K,V)]\n",
    "\n",
    "**Actions:**\n",
    "\n",
    "- *count*() : RDD[T] -> Long\n",
    "- *collect*() : RDD[T] -> Seq[T]\n",
    "- *reduce*(f: (T,T) -> T) : RDD[T] -> T\n",
    "- *lookup*(k : K) : RDD[(K,V)] -> Seq[V] (on hash/range partitionied RDDs)\n",
    "- *save*(path: String) : Outputs RDD to a storage system "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textFile = sc.textFile(\"/repository/complete-shakespeare.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/repository/complete-shakespeare.txt MapPartitionsRDD[3] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.1 ms, sys: 4.31 ms, total: 16.4 ms\n",
      "Wall time: 2.96 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "124796"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "textFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordcount = textFile.flatMap(lambda line: line.split(\" \")) \\\n",
    "            .map(lambda word: (word, 1)) \\\n",
    "            .reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[17] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o114.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://dsci/user/lngo/output-wordcount-01 already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1191)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1168)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1168)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1168)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1071)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1037)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1037)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1037)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:963)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:963)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:963)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1489)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1468)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1468)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1468)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:550)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-619089ad4eaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwordcount\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"output-wordcount-01\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/hdp/2.6.0.3-8/spark2/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msaveAsTextFile\u001b[0;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[1;32m   1550\u001b[0m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompressionCodec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m     \u001b[0;31m# Pair functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/2.6.0.3-8/spark2/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/2.6.0.3-8/spark2/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o114.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://dsci/user/lngo/output-wordcount-01 already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1191)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1168)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1168)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1168)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1071)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1037)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1037)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1037)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:963)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:963)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:963)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1489)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1468)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1468)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1468)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:550)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "wordcount.saveAsTextFile(\"output-wordcount-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('', 516839)\r\n",
      "('Quince', 1)\r\n",
      "('LIBRARY,', 218)\r\n",
      "('Just', 10)\r\n",
      "('enrooted', 1)\r\n",
      "('divers', 20)\r\n",
      "('Doubtless', 2)\r\n",
      "('undistinguishable,', 1)\r\n",
      "('Rheims,', 1)\r\n",
      "('Freedom!', 1)\r\n",
      "('incorporate.', 1)\r\n",
      "('bawd!', 3)\r\n",
      "('Sir-I', 1)\r\n",
      "('withering', 2)\r\n",
      "('Mopsa,', 1)\r\n",
      "('[BEROWNE', 3)\r\n",
      "('forgetfulness?', 1)\r\n",
      "('Tranio?', 1)\r\n",
      "('Wound', 3)\r\n",
      "('twice,', 2)\r\n"
     ]
    }
   ],
   "source": [
    "!cat output-wordcount-01/part-00000 \\\n",
    "    2>/dev/null | head -n 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step-by-step actions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1609\r\n",
      "\r\n",
      "THE SONNETS\r\n",
      "\r\n",
      "by William Shakespeare\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                     1\r\n",
      "  From fairest creatures we desire increase,\r\n"
     ]
    }
   ],
   "source": [
    "!cat text/gutenberg-shakespeare.txt \\\n",
    "    2>/dev/null | head -n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordcount_step_01 = textFile.flatMap(lambda line: line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[21] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcount_step_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Project',\n",
       " 'Gutenberg',\n",
       " 'EBook',\n",
       " 'of',\n",
       " 'The',\n",
       " 'Complete',\n",
       " 'Works',\n",
       " 'of',\n",
       " 'William',\n",
       " 'Shakespeare,',\n",
       " 'by',\n",
       " '',\n",
       " 'William',\n",
       " 'Shakespeare',\n",
       " '',\n",
       " 'This',\n",
       " 'eBook',\n",
       " 'is',\n",
       " 'for']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcount_step_01.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordcount_step_02 = wordcount_step_01.map(lambda word: (word, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 1),\n",
       " ('Project', 1),\n",
       " ('Gutenberg', 1),\n",
       " ('EBook', 1),\n",
       " ('of', 1),\n",
       " ('The', 1),\n",
       " ('Complete', 1),\n",
       " ('Works', 1),\n",
       " ('of', 1),\n",
       " ('William', 1),\n",
       " ('Shakespeare,', 1),\n",
       " ('by', 1),\n",
       " ('', 1),\n",
       " ('William', 1),\n",
       " ('Shakespeare', 1),\n",
       " ('', 1),\n",
       " ('This', 1),\n",
       " ('eBook', 1),\n",
       " ('is', 1),\n",
       " ('for', 1)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcount_step_02.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordcount_step_03 = wordcount_step_02.reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 506672),\n",
       " ('Quince', 1),\n",
       " ('LIBRARY,', 221),\n",
       " ('Just', 10),\n",
       " ('enrooted', 1),\n",
       " ('divers', 20),\n",
       " ('Doubtless', 2),\n",
       " ('undistinguishable,', 1),\n",
       " ('Rheims,', 1),\n",
       " ('Freedom!', 1),\n",
       " ('incorporate.', 1),\n",
       " ('bawd!', 3),\n",
       " ('Sir-I', 1),\n",
       " ('withering', 2),\n",
       " ('Mopsa,', 1),\n",
       " ('[BEROWNE', 3),\n",
       " ('forgetfulness?', 1),\n",
       " ('Tranio?', 1),\n",
       " ('Wound', 3),\n",
       " ('twice,', 2)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcount_step_03.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Movie Ratings\n",
    "\n",
    "An independent movie company is looking to invest in a new movie project. With limited finance, the company wants to \n",
    "analyze the reaction of audiences, particularly toward various movie genres, in order to identify beneficial \n",
    "movie project to focus on. The company relies on data collected from a publicly available recommendation service \n",
    "by [MovieLens](http://dl.acm.org/citation.cfm?id=2827872). This \n",
    "[dataset](http://files.grouplens.org/datasets/movielens/ml-10m-README.html) contains **24404096** ratings and **668953**\n",
    " tag applications across **40110** movies. These data were created by **247753** users between January 09, 1995 and January 29, 2016. This dataset was generated on October 17, 2016. \n",
    "\n",
    "From this dataset, several analyses are possible, include the followings:\n",
    "1.   Find movies which have the highest average ratings over the years and identify the corresponding genre.\n",
    "2.   Find genres which have the highest average ratings over the years.\n",
    "3.   Find users who rate movies most frequently in order to contact them for in-depth marketing analysis.\n",
    "\n",
    "These types of analyses, which are somewhat ambiguous, demand the ability to quickly process large amount of data in \n",
    "elatively short amount of time for decision support purposes. In these situations, the sizes of the data typically \n",
    "make analysis done on a single machine impossible and analysis done using a remote storage system impractical. For \n",
    "remainder of the lessons, we will learn how HDFS provides the basis to store massive amount of data and to enable \n",
    "the programming approach to analyze these data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genome-scores.csv  links.csv   ratings.csv  tags.csv\r\n",
      "genome-tags.csv    movies.csv  README.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -h movielens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary\r\n",
      "=======\r\n",
      "\r\n",
      "This dataset (ml-latest) describes 5-star rating and free-text tagging activity from [MovieLens](http://movielens.org), a movie recommendation service. It contains 24404096 ratings and 668953 tag applications across 40110 movies. These data were created by 259137 users between January 09, 1995 and October 17, 2016. This dataset was generated on October 18, 2016.\r\n",
      "\r\n",
      "Users were selected at random for inclusion. All selected users had rated at least 1 movies. No demographic information is included. Each user is represented by an id, and no other information is provided.\r\n",
      "\r\n",
      "The data are contained in the files `genome-scores.csv`, `genome-tags.csv`, `links.csv`, `movies.csv`, `ratings.csv` and `tags.csv`. More details about the contents and use of all these files follows.\r\n",
      "\r\n",
      "This is a *development* dataset. As such, it may change over time and is not an appropriate dataset for shared research results. See available *benchmark* datasets if that is your intent.\r\n",
      "\r\n",
      "This and other GroupLens data sets are publicly available for download at <http://grouplens.org/datasets/>.\r\n",
      "\r\n",
      "\r\n",
      "Usage License\r\n",
      "=============\r\n",
      "\r\n",
      "Neither the University of Minnesota nor any of the researchers involved can guarantee the correctness of the data, its suitability for any particular purpose, or the validity of results based on the use of the data set. The data set may be used for any research purposes under the following conditions:\r\n",
      "\r\n",
      "* The user may not state or imply any endorsement from the University of Minnesota or the GroupLens Research Group.\r\n",
      "* The user must acknowledge the use of the data set in publications resulting from the use of the data set (see below for citation information).\r\n",
      "* The user may not redistribute the data without separate permission.\r\n",
      "* The user may not use this information for any commercial or revenue-bearing purposes without first obtaining permission from a faculty member of the GroupLens Research Project at the University of Minnesota.\r\n",
      "* The executable software scripts are provided \"as is\" without warranty of any kind, either expressed or implied, including, but not limited to, the implied warranties of merchantability and fitness for a particular purpose. The entire risk as to the quality and performance of them is with you. Should the program prove defective, you assume the cost of all necessary servicing, repair or correction.\r\n",
      "\r\n",
      "In no event shall the University of Minnesota, its affiliates or employees be liable to you for any damages arising out of the use or inability to use these programs (including but not limited to loss of data or data being rendered inaccurate).\r\n",
      "\r\n",
      "If you have any further questions or comments, please email <grouplens-info@umn.edu>\r\n",
      "\r\n",
      "\r\n",
      "Citation\r\n",
      "========\r\n",
      "\r\n",
      "To acknowledge use of the dataset in publications, please cite the following paper:\r\n",
      "\r\n",
      "> F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages. DOI=<http://dx.doi.org/10.1145/2827872>\r\n",
      "\r\n",
      "\r\n",
      "Further Information About GroupLens\r\n",
      "===================================\r\n",
      "\r\n",
      "GroupLens is a research group in the Department of Computer Science and Engineering at the University of Minnesota. Since its inception in 1992, GroupLens's research projects have explored a variety of fields including:\r\n",
      "\r\n",
      "* recommender systems\r\n",
      "* online communities\r\n",
      "* mobile and ubiquitious technologies\r\n",
      "* digital libraries\r\n",
      "* local geographic information systems\r\n",
      "\r\n",
      "GroupLens Research operates a movie recommender based on collaborative filtering, MovieLens, which is the source of these data. We encourage you to visit <http://movielens.org> to try it out! If you have exciting ideas for experimental work to conduct on MovieLens, send us an email at <grouplens-info@cs.umn.edu> - we are always interested in working with external collaborators.\r\n",
      "\r\n",
      "\r\n",
      "Content and Use of Files\r\n",
      "========================\r\n",
      "\r\n",
      "Formatting and Encoding\r\n",
      "-----------------------\r\n",
      "\r\n",
      "The dataset files are written as [comma-separated values](http://en.wikipedia.org/wiki/Comma-separated_values) files with a single header row. Columns that contain commas (`,`) are escaped using double-quotes (`\"`). These files are encoded as UTF-8. If accented characters in movie titles or tag values (e.g. Mis√©rables, Les (1995)) display incorrectly, make sure that any program reading the data, such as a text editor, terminal, or script, is configured for UTF-8.\r\n",
      "\r\n",
      "User Ids\r\n",
      "--------\r\n",
      "\r\n",
      "MovieLens users were selected at random for inclusion. Their ids have been anonymized. User ids are consistent between `ratings.csv` and `tags.csv` (i.e., the same id refers to the same user across the two files).\r\n",
      "\r\n",
      "Movie Ids\r\n",
      "---------\r\n",
      "\r\n",
      "Only movies with at least one rating or tag are included in the dataset. These movie ids are consistent with those used on the MovieLens web site (e.g., id `1` corresponds to the URL <https://movielens.org/movies/1>). Movie ids are consistent between `ratings.csv`, `tags.csv`, `movies.csv`, and `links.csv` (i.e., the same id refers to the same movie across these four data files).\r\n",
      "\r\n",
      "\r\n",
      "Ratings Data File Structure (ratings.csv)\r\n",
      "-----------------------------------------\r\n",
      "\r\n",
      "All ratings are contained in the file `ratings.csv`. Each line of this file after the header row represents one rating of one movie by one user, and has the following format:\r\n",
      "\r\n",
      "    userId,movieId,rating,timestamp\r\n",
      "\r\n",
      "The lines within this file are ordered first by userId, then, within user, by movieId.\r\n",
      "\r\n",
      "Ratings are made on a 5-star scale, with half-star increments (0.5 stars - 5.0 stars).\r\n",
      "\r\n",
      "Timestamps represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970.\r\n",
      "\r\n",
      "Tags Data File Structure (tags.csv)\r\n",
      "-----------------------------------\r\n",
      "\r\n",
      "All tags are contained in the file `tags.csv`. Each line of this file after the header row represents one tag applied to one movie by one user, and has the following format:\r\n",
      "\r\n",
      "    userId,movieId,tag,timestamp\r\n",
      "\r\n",
      "The lines within this file are ordered first by userId, then, within user, by movieId.\r\n",
      "\r\n",
      "Tags are user-generated metadata about movies. Each tag is typically a single word or short phrase. The meaning, value, and purpose of a particular tag is determined by each user.\r\n",
      "\r\n",
      "Timestamps represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970.\r\n",
      "\r\n",
      "Movies Data File Structure (movies.csv)\r\n",
      "---------------------------------------\r\n",
      "\r\n",
      "Movie information is contained in the file `movies.csv`. Each line of this file after the header row represents one movie, and has the following format:\r\n",
      "\r\n",
      "    movieId,title,genres\r\n",
      "\r\n",
      "Movie titles are entered manually or imported from <https://www.themoviedb.org/>, and include the year of release in parentheses. Errors and inconsistencies may exist in these titles.\r\n",
      "\r\n",
      "Genres are a pipe-separated list, and are selected from the following:\r\n",
      "\r\n",
      "* Action\r\n",
      "* Adventure\r\n",
      "* Animation\r\n",
      "* Children's\r\n",
      "* Comedy\r\n",
      "* Crime\r\n",
      "* Documentary\r\n",
      "* Drama\r\n",
      "* Fantasy\r\n",
      "* Film-Noir\r\n",
      "* Horror\r\n",
      "* Musical\r\n",
      "* Mystery\r\n",
      "* Romance\r\n",
      "* Sci-Fi\r\n",
      "* Thriller\r\n",
      "* War\r\n",
      "* Western\r\n",
      "* (no genres listed)\r\n",
      "\r\n",
      "Links Data File Structure (links.csv)\r\n",
      "---------------------------------------\r\n",
      "\r\n",
      "Identifiers that can be used to link to other sources of movie data are contained in the file `links.csv`. Each line of this file after the header row represents one movie, and has the following format:\r\n",
      "\r\n",
      "    movieId,imdbId,tmdbId\r\n",
      "\r\n",
      "movieId is an identifier for movies used by <https://movielens.org>. E.g., the movie Toy Story has the link <https://movielens.org/movies/1>.\r\n",
      "\r\n",
      "imdbId is an identifier for movies used by <http://www.imdb.com>. E.g., the movie Toy Story has the link <http://www.imdb.com/title/tt0114709/>.\r\n",
      "\r\n",
      "tmdbId is an identifier for movies used by <https://www.themoviedb.org>. E.g., the movie Toy Story has the link <https://www.themoviedb.org/movie/862>.\r\n",
      "\r\n",
      "Use of the resources listed above is subject to the terms of each provider.\r\n",
      "\r\n",
      "Tag Genome (genome-scores.csv and genome-tags.csv)\r\n",
      "-------------------------------------------------\r\n",
      "\r\n",
      "This data set includes a current copy of the Tag Genome.\r\n",
      "\r\n",
      "[genome-paper]: http://files.grouplens.org/papers/tag_genome.pdf\r\n",
      "\r\n",
      "The tag genome is a data structure that contains tag relevance scores for movies.  The structure is a dense matrix: each movie in the genome has a value for *every* tag in the genome.\r\n",
      "\r\n",
      "As described in [this article][genome-paper], the tag genome encodes how strongly movies exhibit particular properties represented by tags (atmospheric, thought-provoking, realistic, etc.). The tag genome was computed using a machine learning algorithm on user-contributed content including tags, ratings, and textual reviews.\r\n",
      "\r\n",
      "The genome is split into two files.  The file `genome-scores.csv` contains movie-tag relevance data in the following format:\r\n",
      "\r\n",
      "    movieId,tagId,relevance\r\n",
      "\r\n",
      "The second file, `genome-tags.csv`, provides the tag descriptions for the tag IDs in the genome file, in the following format:\r\n",
      "\r\n",
      "    tagId,tag\r\n",
      "\r\n",
      "The `tagId` values are generated when the data set is exported, so they may vary from version to version of the MovieLens data sets.\r\n",
      "\r\n",
      "Cross-Validation\r\n",
      "----------------\r\n",
      "\r\n",
      "Prior versions of the MovieLens dataset included either pre-computed cross-folds or scripts to perform this computation. We no longer bundle either of these features with the dataset, since most modern toolkits provide this as a built-in feature. If you wish to learn about standard approaches to cross-fold computation in the context of recommender systems evaluation, see [LensKit](http://lenskit.org) for tools, documentation, and open-source code examples.\r\n"
     ]
    }
   ],
   "source": [
    "!cat  movielens/README.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movieId,imdbId,tmdbId\r",
      "\r\n",
      "1,0114709,862\r",
      "\r\n",
      "2,0113497,8844\r",
      "\r\n",
      "3,0113228,15602\r",
      "\r\n",
      "4,0114885,31357\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat  movielens/links.csv \\\n",
    "    2>/dev/null | head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movieId,title,genres\r",
      "\r\n",
      "1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy\r",
      "\r\n",
      "2,Jumanji (1995),Adventure|Children|Fantasy\r",
      "\r\n",
      "3,Grumpier Old Men (1995),Comedy|Romance\r",
      "\r\n",
      "4,Waiting to Exhale (1995),Comedy|Drama|Romance\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat  movielens/movies.csv \\\n",
    "    2>/dev/null | head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userId,movieId,rating,timestamp\r",
      "\r\n",
      "1,122,2.0,945544824\r",
      "\r\n",
      "1,172,1.0,945544871\r",
      "\r\n",
      "1,1221,5.0,945544788\r",
      "\r\n",
      "1,1441,4.0,945544871\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat  movielens/ratings.csv \\\n",
    "    2>/dev/null | head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userId,movieId,tag,timestamp\r",
      "\r\n",
      "28,63062,angelina jolie,1263047558\r",
      "\r\n",
      "40,4973,Poetic,1436439070\r",
      "\r\n",
      "40,117533,privacy,1436439140\r",
      "\r\n",
      "57,356,life positive,1291771526\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat  movielens/tags.csv \\\n",
    "    2>/dev/null | head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ratings = sc.textFile(\"/repository/movielens/ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/repository/movielens/ratings.csv MapPartitionsRDD[33] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.1 ms, sys: 4.38 ms, total: 14.5 ms\n",
      "Wall time: 13.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24404097"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ratings.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9 ms, sys: 4 ms, total: 13 ms\n",
      "Wall time: 2.99 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24404097"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ratings.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9 ms, sys: 3 ms, total: 12 ms\n",
      "Wall time: 3.05 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24404097"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ratings.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Find movies which have the highest average ratings over the years and identify the corresponding genre\n",
    "\n",
    "- Find the average ratings of all movies over the years\n",
    "- Identify the corresponding genres for each movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['userId,movieId,rating,timestamp',\n",
       " '1,122,2.0,945544824',\n",
       " '1,172,1.0,945544871',\n",
       " '1,1221,5.0,945544788',\n",
       " '1,1441,4.0,945544871']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userId,movieId,rating,timestamp\n"
     ]
    }
   ],
   "source": [
    "ratingHeader = ratings.first() #extract header\n",
    "print(ratingHeader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratingsOnly = ratings.filter(lambda x:x != ratingHeader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,122,2.0,945544824',\n",
       " '1,172,1.0,945544871',\n",
       " '1,1221,5.0,945544788',\n",
       " '1,1441,4.0,945544871',\n",
       " '1,1609,3.0,945544824']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratingsOnly.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movieRatings = ratingsOnly.map(lambda line: (line.split(\",\")[1], float(line.split(\",\")[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('122', 2.0), ('172', 1.0), ('1221', 5.0), ('1441', 4.0), ('1609', 3.0)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movieRatings.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Possible approaches in aggregating data:** \n",
    "- groupByKey and mapValues\n",
    "- reduceByKey and countByKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**groupByKey and mapValues**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('154214', <pyspark.resultiterable.ResultIterable at 0x2ba1e24ad588>),\n",
       " ('27479', <pyspark.resultiterable.ResultIterable at 0x2ba1e24ad710>),\n",
       " ('129667', <pyspark.resultiterable.ResultIterable at 0x2ba1e24adf28>),\n",
       " ('140054', <pyspark.resultiterable.ResultIterable at 0x2ba1e24ad780>),\n",
       " ('45183', <pyspark.resultiterable.ResultIterable at 0x2ba1e24ade10>)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupByKeyRatings = movieRatings.groupByKey()\n",
    "\n",
    "groupByKeyRatings.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('154214', [3.5, 0.5, 2.0]),\n",
       " ('27479', [4.0, 1.5, 1.0]),\n",
       " ('129667', [5.0, 5.0, 2.0, 3.0]),\n",
       " ('140054', [3.5, 2.0, 4.0]),\n",
       " ('45183',\n",
       "  [3.0,\n",
       "   1.5,\n",
       "   3.5,\n",
       "   3.0,\n",
       "   3.0,\n",
       "   2.0,\n",
       "   3.5,\n",
       "   2.0,\n",
       "   1.5,\n",
       "   3.5,\n",
       "   4.0,\n",
       "   4.0,\n",
       "   3.5,\n",
       "   5.0,\n",
       "   3.5,\n",
       "   4.0,\n",
       "   0.5,\n",
       "   2.0,\n",
       "   4.5,\n",
       "   5.0,\n",
       "   2.0,\n",
       "   4.0,\n",
       "   4.5,\n",
       "   4.0,\n",
       "   5.0,\n",
       "   3.5,\n",
       "   2.0,\n",
       "   3.5,\n",
       "   4.5,\n",
       "   3.5,\n",
       "   0.5,\n",
       "   5.0,\n",
       "   4.5,\n",
       "   3.0,\n",
       "   3.5,\n",
       "   3.5,\n",
       "   4.5,\n",
       "   3.5,\n",
       "   4.5,\n",
       "   4.5,\n",
       "   4.5,\n",
       "   3.5,\n",
       "   5.0,\n",
       "   0.5,\n",
       "   4.0,\n",
       "   4.0,\n",
       "   2.5,\n",
       "   2.5,\n",
       "   2.0,\n",
       "   3.0,\n",
       "   4.5,\n",
       "   3.5,\n",
       "   2.5,\n",
       "   4.0,\n",
       "   3.5,\n",
       "   4.0,\n",
       "   3.0,\n",
       "   2.5,\n",
       "   4.0,\n",
       "   4.5,\n",
       "   4.5,\n",
       "   4.5,\n",
       "   4.5,\n",
       "   4.5,\n",
       "   4.0,\n",
       "   3.5,\n",
       "   3.0,\n",
       "   2.5,\n",
       "   3.5,\n",
       "   4.5,\n",
       "   2.5,\n",
       "   5.0,\n",
       "   3.5,\n",
       "   3.5,\n",
       "   3.0,\n",
       "   2.5,\n",
       "   3.0,\n",
       "   4.5,\n",
       "   2.0,\n",
       "   4.0,\n",
       "   3.5,\n",
       "   3.5,\n",
       "   3.0,\n",
       "   3.5,\n",
       "   3.5,\n",
       "   5.0,\n",
       "   4.5,\n",
       "   3.0,\n",
       "   3.5,\n",
       "   3.0,\n",
       "   3.0,\n",
       "   3.5,\n",
       "   5.0,\n",
       "   5.0,\n",
       "   4.5,\n",
       "   3.5,\n",
       "   4.0,\n",
       "   2.0,\n",
       "   4.5,\n",
       "   3.0,\n",
       "   5.0,\n",
       "   4.0,\n",
       "   3.0,\n",
       "   4.5,\n",
       "   2.5,\n",
       "   4.0,\n",
       "   2.0,\n",
       "   4.0,\n",
       "   4.0,\n",
       "   2.5,\n",
       "   1.0,\n",
       "   3.0,\n",
       "   3.0,\n",
       "   3.5,\n",
       "   5.0,\n",
       "   3.5,\n",
       "   4.0,\n",
       "   3.0,\n",
       "   4.0,\n",
       "   4.5,\n",
       "   4.0,\n",
       "   5.0,\n",
       "   4.0,\n",
       "   3.5,\n",
       "   3.0,\n",
       "   3.0,\n",
       "   4.5,\n",
       "   5.0,\n",
       "   5.0,\n",
       "   3.5,\n",
       "   4.0,\n",
       "   3.5,\n",
       "   2.0,\n",
       "   2.0,\n",
       "   2.5,\n",
       "   3.5,\n",
       "   3.0,\n",
       "   2.0,\n",
       "   5.0,\n",
       "   4.0,\n",
       "   4.0,\n",
       "   3.5,\n",
       "   3.5,\n",
       "   3.5,\n",
       "   5.0,\n",
       "   4.5,\n",
       "   3.5,\n",
       "   4.0,\n",
       "   3.0,\n",
       "   3.5,\n",
       "   4.5,\n",
       "   3.5,\n",
       "   5.0,\n",
       "   4.5,\n",
       "   3.0,\n",
       "   2.0,\n",
       "   4.0,\n",
       "   3.0,\n",
       "   1.0,\n",
       "   3.5,\n",
       "   4.0,\n",
       "   5.0,\n",
       "   3.5,\n",
       "   4.0,\n",
       "   3.0,\n",
       "   3.5,\n",
       "   3.0,\n",
       "   3.5,\n",
       "   4.0,\n",
       "   4.5,\n",
       "   4.5,\n",
       "   4.0,\n",
       "   2.5,\n",
       "   4.0,\n",
       "   3.0,\n",
       "   3.0,\n",
       "   3.5,\n",
       "   4.0,\n",
       "   3.0,\n",
       "   4.0,\n",
       "   4.0,\n",
       "   4.0,\n",
       "   3.0,\n",
       "   4.0,\n",
       "   4.5,\n",
       "   3.0,\n",
       "   1.0,\n",
       "   4.0,\n",
       "   3.0,\n",
       "   1.0,\n",
       "   4.0,\n",
       "   4.0,\n",
       "   2.0,\n",
       "   4.0,\n",
       "   1.5,\n",
       "   3.5,\n",
       "   3.0,\n",
       "   4.5,\n",
       "   4.0,\n",
       "   1.0,\n",
       "   2.5,\n",
       "   4.5,\n",
       "   4.0,\n",
       "   3.0,\n",
       "   2.5,\n",
       "   3.0,\n",
       "   3.5,\n",
       "   3.0,\n",
       "   3.5,\n",
       "   3.5,\n",
       "   2.5,\n",
       "   3.0,\n",
       "   4.0,\n",
       "   3.5,\n",
       "   4.0,\n",
       "   4.0,\n",
       "   0.5,\n",
       "   3.5,\n",
       "   3.0,\n",
       "   4.0,\n",
       "   5.0,\n",
       "   3.0,\n",
       "   3.0,\n",
       "   3.5,\n",
       "   2.0,\n",
       "   4.5,\n",
       "   3.5,\n",
       "   4.5,\n",
       "   4.5,\n",
       "   3.5,\n",
       "   4.5,\n",
       "   4.5,\n",
       "   4.0,\n",
       "   2.0,\n",
       "   4.0,\n",
       "   4.5,\n",
       "   4.0,\n",
       "   4.5,\n",
       "   4.0,\n",
       "   2.5,\n",
       "   4.0,\n",
       "   4.5,\n",
       "   3.0,\n",
       "   3.5,\n",
       "   3.0,\n",
       "   1.0,\n",
       "   5.0,\n",
       "   4.5,\n",
       "   2.0,\n",
       "   4.0,\n",
       "   5.0,\n",
       "   2.0,\n",
       "   3.5,\n",
       "   3.5,\n",
       "   3.0,\n",
       "   2.5,\n",
       "   4.5,\n",
       "   4.0,\n",
       "   3.5,\n",
       "   4.0,\n",
       "   0.5,\n",
       "   3.5,\n",
       "   4.0,\n",
       "   4.0,\n",
       "   3.5,\n",
       "   1.5,\n",
       "   4.0,\n",
       "   4.5,\n",
       "   4.0,\n",
       "   3.5,\n",
       "   4.0,\n",
       "   3.5,\n",
       "   3.5,\n",
       "   3.5,\n",
       "   4.0,\n",
       "   3.5,\n",
       "   3.0,\n",
       "   4.5,\n",
       "   3.5,\n",
       "   2.5,\n",
       "   3.5,\n",
       "   4.0,\n",
       "   4.0,\n",
       "   5.0,\n",
       "   4.0,\n",
       "   3.0,\n",
       "   3.0,\n",
       "   3.5,\n",
       "   4.0,\n",
       "   3.5,\n",
       "   3.5,\n",
       "   2.0,\n",
       "   3.5,\n",
       "   2.5,\n",
       "   2.0,\n",
       "   4.0,\n",
       "   2.5,\n",
       "   4.5,\n",
       "   3.0,\n",
       "   3.5,\n",
       "   4.0,\n",
       "   4.0,\n",
       "   3.5,\n",
       "   5.0,\n",
       "   0.5,\n",
       "   4.0,\n",
       "   3.5,\n",
       "   2.5,\n",
       "   4.0,\n",
       "   0.5,\n",
       "   3.5,\n",
       "   5.0,\n",
       "   5.0,\n",
       "   1.5,\n",
       "   4.0,\n",
       "   3.5,\n",
       "   2.0,\n",
       "   5.0,\n",
       "   1.5,\n",
       "   3.5,\n",
       "   5.0,\n",
       "   4.0,\n",
       "   3.5,\n",
       "   3.0,\n",
       "   2.0,\n",
       "   3.5,\n",
       "   4.0,\n",
       "   4.5,\n",
       "   4.5,\n",
       "   3.0,\n",
       "   4.5,\n",
       "   3.0,\n",
       "   2.0,\n",
       "   5.0,\n",
       "   3.5,\n",
       "   4.0,\n",
       "   3.0,\n",
       "   4.0,\n",
       "   3.5,\n",
       "   5.0,\n",
       "   3.5,\n",
       "   4.0,\n",
       "   3.5,\n",
       "   4.5,\n",
       "   4.0,\n",
       "   4.0,\n",
       "   4.5,\n",
       "   5.0,\n",
       "   4.5,\n",
       "   3.5,\n",
       "   3.5,\n",
       "   3.0,\n",
       "   4.0,\n",
       "   3.5,\n",
       "   3.0,\n",
       "   5.0,\n",
       "   3.5,\n",
       "   4.0,\n",
       "   4.5,\n",
       "   4.0,\n",
       "   3.0,\n",
       "   3.0,\n",
       "   4.0,\n",
       "   4.0,\n",
       "   2.5,\n",
       "   3.5,\n",
       "   3.5,\n",
       "   4.5,\n",
       "   3.5,\n",
       "   4.0,\n",
       "   4.5,\n",
       "   3.5,\n",
       "   4.0,\n",
       "   3.0,\n",
       "   5.0,\n",
       "   3.5,\n",
       "   5.0,\n",
       "   5.0,\n",
       "   4.0,\n",
       "   4.0,\n",
       "   4.0,\n",
       "   4.0,\n",
       "   5.0,\n",
       "   3.0,\n",
       "   3.0,\n",
       "   5.0,\n",
       "   3.5,\n",
       "   4.5,\n",
       "   3.0,\n",
       "   3.0,\n",
       "   3.5,\n",
       "   4.0,\n",
       "   4.0,\n",
       "   2.0,\n",
       "   4.0,\n",
       "   5.0,\n",
       "   4.0,\n",
       "   4.0,\n",
       "   5.0,\n",
       "   3.5,\n",
       "   5.0,\n",
       "   3.0,\n",
       "   3.0,\n",
       "   4.0,\n",
       "   3.5,\n",
       "   0.5,\n",
       "   5.0,\n",
       "   4.0,\n",
       "   2.5,\n",
       "   3.5,\n",
       "   3.5,\n",
       "   3.0,\n",
       "   3.5,\n",
       "   2.5,\n",
       "   3.0,\n",
       "   2.5,\n",
       "   2.5,\n",
       "   4.0,\n",
       "   4.0,\n",
       "   4.5,\n",
       "   4.0,\n",
       "   5.0])]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapValuesToListRatings = groupByKeyRatings.mapValues(list)\n",
    "\n",
    "mapValuesToListRatings.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('154214', 2.0),\n",
       " ('27479', 2.1666666666666665),\n",
       " ('129667', 3.75),\n",
       " ('140054', 3.1666666666666665),\n",
       " ('45183', 3.5485781990521326)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avgRatings01 = mapValuesToListRatings.mapValues(lambda V: sum(V) / float(len(V)))\n",
    "\n",
    "avgRatings01.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(3.5 + 3.5 + 2.5 + 3.5 + 2.0 + 3.5 + 2.5 + 3.0) / 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**reduceByKey and countByKey**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "countsByKey = movieRatings.countByKey()\n",
    "\n",
    "countsByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sumValues(x,y):\n",
    "    return (x + y)\n",
    "\n",
    "sumRatings = movieRatings.reduceByKey(sumValues)\n",
    "\n",
    "sumRatings.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "sumRatings = movieRatings.reduceByKey(operator.add)\n",
    "sumRatings.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avgRatings02 = sumRatings.map(lambda x: (x[0], x[1] / countsByKey.get(x[0])))\n",
    "\n",
    "avgRatings02.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we augment movie ratings data with title informations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movies = sc.textFile(\"movielens/movies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movieId,title,genres\n"
     ]
    }
   ],
   "source": [
    "movieHeader = movies.first() #extract header\n",
    "print(movieHeader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy',\n",
       " '2,Jumanji (1995),Adventure|Children|Fantasy',\n",
       " '3,Grumpier Old Men (1995),Comedy|Romance',\n",
       " '4,Waiting to Exhale (1995),Comedy|Drama|Romance',\n",
       " '5,Father of the Bride Part II (1995),Comedy']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies = movies.filter(lambda x:x != movieHeader)\n",
    "\n",
    "movies.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', ('Toy Story (1995)', 'Adventure|Animation|Children|Comedy|Fantasy')),\n",
       " ('2', ('Jumanji (1995)', 'Adventure|Children|Fantasy')),\n",
       " ('3', ('Grumpier Old Men (1995)', 'Comedy|Romance')),\n",
       " ('4', ('Waiting to Exhale (1995)', 'Comedy|Drama|Romance')),\n",
       " ('5', ('Father of the Bride Part II (1995)', 'Comedy'))]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movieInfo = movies.map(lambda line: (line.split(\",\")[0], (line.split(\",\")[1], line.split(\",\")[2])))\n",
    "\n",
    "movieInfo.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1440', (2.776470588235294, ('Amos & Andrew (1993)', 'Comedy'))),\n",
       " ('106450', (4.25, ('Chicago Overcoat (2009)', 'Action|Drama'))),\n",
       " ('564', (2.3294797687861273, ('Chasers (1994)', 'Comedy'))),\n",
       " ('108318', (2.8088235294117645, ('\"Single Shot', ' A (2013)\"'))),\n",
       " ('150421', (3.0, ('Man on Horseback (1969)', '(no genres listed)')))]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmentedRatings = avgRatings01.join(movieInfo)\n",
    "\n",
    "augmentedRatings.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Movie with highest average rating:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('120436', (5.0, ('Garbo Talks (1984)', 'Comedy|Drama'))),\n",
       " ('136874', (5.0, ('Natarang (2010)', '(no genres listed)'))),\n",
       " ('146946', (5.0, ('The Hardy Bucks Movie (2013)', 'Comedy'))),\n",
       " ('114353', (5.0, ('Heavyweights (Schwere Jungs) (2006)', 'Comedy'))),\n",
       " ('123727', (5.0, ('Immigration Tango (2011)', 'Comedy|Romance'))),\n",
       " ('164869', (5.0, ('A Cinderella Story: If the Shoe Fits (2016)', 'Comedy'))),\n",
       " ('159423',\n",
       "  (5.0,\n",
       "   ('Jonas Brothers: The Concert Experience (2009)', '(no genres listed)'))),\n",
       " ('133575', (5.0, ('Do Detectives Think? (1927)', 'Comedy'))),\n",
       " ('93967',\n",
       "  (5.0, ('\"Keeping the Promise (Sign of the Beaver', ' The) (1997)\"'))),\n",
       " ('160325', (5.0, ('I Love Hong Kong (2011)', 'Comedy')))]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmentedRatings.takeOrdered(10, key = lambda x : -x[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Movie with lowest average rating:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('164927', (0.5, ('Where Souls Go (2007)', '(no genres listed)'))),\n",
       " ('156840', (0.5, ('Jurassic Attack (2012)', 'Action|Sci-Fi'))),\n",
       " ('138008', (0.5, ('New Year (2011)', '(no genres listed)'))),\n",
       " ('131152', (0.5, ('The Fat Spy (1966)', 'Comedy'))),\n",
       " ('109355', (0.5, ('13 Fighting Men (1960)', 'Western'))),\n",
       " ('127327', (0.5, ('Khan Kluay (2006)', 'Adventure|Animation|Children'))),\n",
       " ('160978', (0.5, ('Hellevator (2004)', 'Horror|Sci-Fi'))),\n",
       " ('145285', (0.5, ('Octopus (2000)', 'Action|Horror|Thriller'))),\n",
       " ('133541', (0.5, ('Two Hundred Thousand Dirty (2014)', 'Comedy'))),\n",
       " ('139717', (0.5, ('10 Cent Pistol (2015)', 'Crime|Thriller')))]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmentedRatings.takeOrdered(10, key = lambda x : x[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "- Augment the mapping process of WordCount with a function to filter out punctuations and capitalization from the unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge:\n",
    "\n",
    "1. Make appropriate changes so that only movies with averaged ratings higher than 3.75 are collected\n",
    "2. Further enhance your modification so that only movies with averaged ratings higher than 3.75 and number of ratings of at least 1000 times are collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Find genres which have the highest average ratings over the years\n",
    "\n",
    "- Identify the genres associated with a movie and its rating\n",
    "- Each movie can have multiple genres. How to flip the Key/Value pair?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('122', 2.0), ('172', 1.0), ('1221', 5.0), ('1441', 4.0), ('1609', 3.0)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movieRatings.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', ('Toy Story (1995)', 'Adventure|Animation|Children|Comedy|Fantasy')),\n",
       " ('2', ('Jumanji (1995)', 'Adventure|Children|Fantasy')),\n",
       " ('3', ('Grumpier Old Men (1995)', 'Comedy|Romance')),\n",
       " ('4', ('Waiting to Exhale (1995)', 'Comedy|Drama|Romance')),\n",
       " ('5', ('Father of the Bride Part II (1995)', 'Comedy'))]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movieInfo.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "augmentedInfo = movieRatings.join(movieInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1440', (3.0, ('Amos & Andrew (1993)', 'Comedy'))),\n",
       " ('1440', (2.0, ('Amos & Andrew (1993)', 'Comedy'))),\n",
       " ('1440', (3.0, ('Amos & Andrew (1993)', 'Comedy'))),\n",
       " ('1440', (5.0, ('Amos & Andrew (1993)', 'Comedy'))),\n",
       " ('1440', (3.0, ('Amos & Andrew (1993)', 'Comedy')))]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmentedInfo.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Adventure', 3.0), ('Animation', 3.0), ('Children', 3.0), ('Comedy', 3.0), ('Fantasy', 3.0)]\n"
     ]
    }
   ],
   "source": [
    "def extractGenreRating (t):\n",
    "    final_tuples = []\n",
    "    genreList = t[1][1][1].split(\"|\")\n",
    "    for genre in genreList:\n",
    "        final_tuples.append((genre,t[1][0]))\n",
    "    return final_tuples\n",
    "\n",
    "print(extractGenreRating((u'1', (3.0, (u'Toy Story (1995)', u'Adventure|Animation|Children|Comedy|Fantasy')))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "genreRatings = augmentedInfo.flatMap(extractGenreRating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Comedy', 3.0),\n",
       " ('Comedy', 2.0),\n",
       " ('Comedy', 3.0),\n",
       " ('Comedy', 5.0),\n",
       " ('Comedy', 3.0)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genreRatings.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge:\n",
    "\n",
    "Complete the remaining portion of task 2.2: Calculating the average rating of each genre over the years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Find users who rate movies most frequently in order to contact them for in-depth marketing analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How do you define \"frequently\"?\n",
    "    - At least once per week?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "userRatings = ratingsOnly.map(lambda line: (line.split(\",\")[0], float(line.split(\",\")[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('51313',\n",
       "  [847620176.0,\n",
       "   847620106.0,\n",
       "   847620374.0,\n",
       "   847620326.0,\n",
       "   847620411.0,\n",
       "   847620264.0,\n",
       "   847620347.0,\n",
       "   847619964.0,\n",
       "   847619965.0,\n",
       "   847620326.0,\n",
       "   847620233.0,\n",
       "   847620294.0,\n",
       "   847620021.0,\n",
       "   847620211.0,\n",
       "   847620021.0,\n",
       "   847620326.0,\n",
       "   847619878.0,\n",
       "   847620211.0,\n",
       "   847620264.0,\n",
       "   847620021.0,\n",
       "   847620374.0,\n",
       "   847620470.0,\n",
       "   847619923.0,\n",
       "   847620049.0,\n",
       "   847620049.0,\n",
       "   847620347.0,\n",
       "   847620411.0,\n",
       "   847620049.0,\n",
       "   847620072.0,\n",
       "   847620176.0,\n",
       "   847620211.0,\n",
       "   847620690.0,\n",
       "   847619900.0,\n",
       "   847620748.0,\n",
       "   847620748.0,\n",
       "   847620049.0,\n",
       "   847620233.0,\n",
       "   847620106.0,\n",
       "   847620656.0,\n",
       "   847620072.0,\n",
       "   847619900.0,\n",
       "   847619878.0]),\n",
       " ('23161',\n",
       "  [945058557.0,\n",
       "   945063385.0,\n",
       "   945049906.0,\n",
       "   945063316.0,\n",
       "   945049950.0,\n",
       "   945057901.0,\n",
       "   945057196.0,\n",
       "   945057745.0,\n",
       "   945057745.0,\n",
       "   945058411.0,\n",
       "   945050030.0,\n",
       "   945049753.0,\n",
       "   945063619.0,\n",
       "   945061581.0,\n",
       "   945058080.0,\n",
       "   945057505.0,\n",
       "   945057506.0,\n",
       "   945057314.0,\n",
       "   945061786.0,\n",
       "   945049117.0,\n",
       "   945057745.0,\n",
       "   945057351.0,\n",
       "   945050473.0,\n",
       "   945050180.0,\n",
       "   945063339.0,\n",
       "   945063284.0,\n",
       "   945049906.0,\n",
       "   945057821.0,\n",
       "   945062117.0,\n",
       "   945050149.0,\n",
       "   945050180.0,\n",
       "   945057977.0,\n",
       "   945057314.0,\n",
       "   945049792.0,\n",
       "   945057197.0,\n",
       "   945058302.0,\n",
       "   945309039.0,\n",
       "   945050507.0,\n",
       "   945057538.0,\n",
       "   945049986.0,\n",
       "   945057197.0,\n",
       "   945049986.0,\n",
       "   945050428.0,\n",
       "   945061591.0,\n",
       "   945058277.0,\n",
       "   945063284.0,\n",
       "   945063316.0,\n",
       "   945063511.0,\n",
       "   945050221.0,\n",
       "   945063284.0,\n",
       "   945061819.0,\n",
       "   945057453.0,\n",
       "   945061618.0,\n",
       "   945062117.0,\n",
       "   945063236.0,\n",
       "   945049906.0,\n",
       "   945057417.0,\n",
       "   945061388.0,\n",
       "   945050962.0,\n",
       "   945057745.0,\n",
       "   945061499.0,\n",
       "   945058245.0,\n",
       "   945061521.0,\n",
       "   945049237.0,\n",
       "   945050071.0,\n",
       "   945063183.0,\n",
       "   945050940.0,\n",
       "   945058495.0,\n",
       "   945049754.0,\n",
       "   945049792.0,\n",
       "   945050403.0,\n",
       "   945049792.0,\n",
       "   945050896.0,\n",
       "   945050428.0,\n",
       "   945061581.0,\n",
       "   945050453.0,\n",
       "   945050473.0,\n",
       "   945049906.0,\n",
       "   945049819.0,\n",
       "   945049723.0,\n",
       "   945049819.0,\n",
       "   945050896.0,\n",
       "   945051054.0,\n",
       "   945062059.0,\n",
       "   945050112.0,\n",
       "   945050071.0,\n",
       "   945057351.0,\n",
       "   945050221.0,\n",
       "   945063440.0,\n",
       "   945057977.0,\n",
       "   945062118.0,\n",
       "   945049861.0,\n",
       "   945057314.0,\n",
       "   945061749.0,\n",
       "   945057901.0,\n",
       "   945057868.0,\n",
       "   945061618.0,\n",
       "   945063461.0,\n",
       "   945057821.0,\n",
       "   945050030.0,\n",
       "   945058080.0,\n",
       "   945057417.0,\n",
       "   945063414.0,\n",
       "   945058080.0,\n",
       "   945050030.0,\n",
       "   945049950.0,\n",
       "   945057232.0,\n",
       "   945063385.0,\n",
       "   945061388.0,\n",
       "   945057938.0,\n",
       "   945057417.0,\n",
       "   945057506.0,\n",
       "   945057314.0,\n",
       "   945050428.0,\n",
       "   945050030.0,\n",
       "   945051070.0,\n",
       "   945057868.0,\n",
       "   945057232.0,\n",
       "   945057262.0,\n",
       "   945057262.0,\n",
       "   945049146.0,\n",
       "   945050180.0,\n",
       "   945057821.0,\n",
       "   945057197.0,\n",
       "   945057417.0,\n",
       "   945058040.0,\n",
       "   945057453.0,\n",
       "   945057506.0,\n",
       "   945050287.0,\n",
       "   945057232.0,\n",
       "   945057745.0,\n",
       "   945057417.0,\n",
       "   945063298.0,\n",
       "   945061786.0,\n",
       "   945063385.0,\n",
       "   945049754.0,\n",
       "   945063209.0,\n",
       "   945063284.0,\n",
       "   945050149.0,\n",
       "   945058006.0,\n",
       "   945050221.0,\n",
       "   945049950.0,\n",
       "   945050149.0,\n",
       "   945049646.0,\n",
       "   945049950.0,\n",
       "   945063528.0,\n",
       "   945049610.0,\n",
       "   945063339.0,\n",
       "   945050112.0,\n",
       "   945057829.0,\n",
       "   945063254.0,\n",
       "   945062118.0,\n",
       "   945049553.0,\n",
       "   945309039.0,\n",
       "   945057868.0,\n",
       "   945049861.0,\n",
       "   945049496.0,\n",
       "   945057977.0,\n",
       "   945061800.0,\n",
       "   945057314.0,\n",
       "   945049196.0,\n",
       "   945061581.0,\n",
       "   945049388.0,\n",
       "   945057262.0,\n",
       "   945049418.0,\n",
       "   945049536.0,\n",
       "   945049117.0,\n",
       "   945049449.0,\n",
       "   945063440.0,\n",
       "   945049610.0,\n",
       "   945051032.0,\n",
       "   945057453.0,\n",
       "   945049950.0,\n",
       "   945051032.0,\n",
       "   945309054.0,\n",
       "   945050112.0,\n",
       "   945057538.0,\n",
       "   945049646.0,\n",
       "   945050149.0,\n",
       "   945058557.0]),\n",
       " ('240955',\n",
       "  [861191393.0,\n",
       "   861191466.0,\n",
       "   861191466.0,\n",
       "   862840712.0,\n",
       "   862841184.0,\n",
       "   861191608.0,\n",
       "   861191394.0,\n",
       "   861191392.0,\n",
       "   861191685.0,\n",
       "   861191723.0,\n",
       "   862840951.0,\n",
       "   862840951.0,\n",
       "   861191394.0,\n",
       "   862841184.0,\n",
       "   861191524.0,\n",
       "   861191636.0,\n",
       "   862840789.0,\n",
       "   861191570.0,\n",
       "   862840431.0,\n",
       "   861191466.0,\n",
       "   861191524.0,\n",
       "   861191466.0,\n",
       "   861191466.0,\n",
       "   861191684.0,\n",
       "   861191570.0,\n",
       "   861191524.0,\n",
       "   862841383.0,\n",
       "   862840789.0,\n",
       "   861191466.0,\n",
       "   861191393.0,\n",
       "   861191684.0,\n",
       "   861191764.0,\n",
       "   861191392.0,\n",
       "   861191608.0,\n",
       "   861191524.0,\n",
       "   861191723.0,\n",
       "   861191466.0,\n",
       "   861191524.0,\n",
       "   862840293.0,\n",
       "   861191570.0,\n",
       "   862841489.0,\n",
       "   861191796.0,\n",
       "   861191832.0,\n",
       "   862841489.0,\n",
       "   861192401.0,\n",
       "   861191608.0,\n",
       "   861191685.0,\n",
       "   862841093.0,\n",
       "   861191764.0,\n",
       "   861192401.0,\n",
       "   861191861.0,\n",
       "   861191466.0,\n",
       "   862841184.0,\n",
       "   861191608.0,\n",
       "   861191570.0,\n",
       "   861191723.0]),\n",
       " ('226527', [979403411.0, 979403454.0, 979403454.0, 979403454.0, 979403411.0]),\n",
       " ('207119',\n",
       "  [1013822922.0,\n",
       "   1013822844.0,\n",
       "   1013822871.0,\n",
       "   1013822844.0,\n",
       "   1013822891.0,\n",
       "   1013822871.0,\n",
       "   1013822922.0,\n",
       "   1013822844.0,\n",
       "   1013822871.0,\n",
       "   1013822871.0,\n",
       "   1013822844.0,\n",
       "   1013822922.0,\n",
       "   1013822991.0,\n",
       "   1013823021.0,\n",
       "   1013823045.0,\n",
       "   1013823021.0,\n",
       "   1013822991.0,\n",
       "   1013823068.0,\n",
       "   1013822991.0,\n",
       "   1013823045.0,\n",
       "   1013823021.0,\n",
       "   1013822991.0,\n",
       "   1013823068.0,\n",
       "   1013823046.0,\n",
       "   1013823151.0])]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratingGroupByUsers = userRatings.groupByKey().mapValues(list)\n",
    "ratingGroupByUsers.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('67056', 37.26),\n",
       " ('77986', 3230.665418227216),\n",
       " ('226527', 8.6),\n",
       " ('207119', 12.28),\n",
       " ('53196', 7.011363636363637)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avgRatingFreq = ratingGroupByUsers.mapValues(lambda V: (max(V) - min(V)) / float(len(V)))\n",
    "avgRatingFreq.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = [1346139060.0,\n",
    "   1346139098.0,\n",
    "   1346139113.0,\n",
    "   1346139053.0,\n",
    "   1346139234.0,\n",
    "   1346139006.0,\n",
    "   1346139209.0,\n",
    "   1346139147.0,\n",
    "   1346138998.0,\n",
    "   1346139206.0,\n",
    "   1346139224.0,\n",
    "   1346139174.0,\n",
    "   1346139152.0,\n",
    "   1346139230.0,\n",
    "   1346139181.0,\n",
    "   1346139159.0,\n",
    "   1346139314.0]\n",
    "(max(x) - min(x)) / float(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topUsers = avgRatingFreq.top(10, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('40407', 51121853.75),\n",
       " ('241087', 40917744.72727273),\n",
       " ('54838', 36601016.0),\n",
       " ('248290', 33999095.666666664),\n",
       " ('39601', 33138222.0),\n",
       " ('155302', 33013341.666666668),\n",
       " ('117995', 29406107.25),\n",
       " ('183383', 29210786.666666668),\n",
       " ('121552', 26685917.875),\n",
       " ('74936', 26309319.0)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topUsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. Airlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spark SQL**\n",
    "- Spark module for structured data processing\n",
    "- provide more information about the structure of both the data and the computation being performed for additional optimization\n",
    "- execute SQL queries written using either a basic SQL syntax or HiveQL\n",
    "\n",
    "**DataFrame**\n",
    "- distributed collection of data organized into named columns\n",
    "- conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood\n",
    "- can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x2ba1e24c6be0>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext = pyspark.SQLContext(sc)\n",
    "sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "airlines = sqlContext.read.format(\"com.databricks.spark.csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferschema\", \"true\")\\\n",
    "    .load(\"airlines/data/\")\\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 ms, sys: 2 ms, total: 9 ms\n",
      "Wall time: 1min 7s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "123534969"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "airlines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 ms, sys: 2 ms, total: 3 ms\n",
      "Wall time: 222 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "123534969"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "airlines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: string (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- ArrTime: string (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- ActualElapsedTime: string (nullable = true)\n",
      " |-- CRSElapsedTime: string (nullable = true)\n",
      " |-- AirTime: string (nullable = true)\n",
      " |-- ArrDelay: string (nullable = true)\n",
      " |-- DepDelay: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: string (nullable = true)\n",
      " |-- TaxiIn: string (nullable = true)\n",
      " |-- TaxiOut: string (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: integer (nullable = true)\n",
      " |-- CarrierDelay: string (nullable = true)\n",
      " |-- WeatherDelay: string (nullable = true)\n",
      " |-- NASDelay: string (nullable = true)\n",
      " |-- SecurityDelay: string (nullable = true)\n",
      " |-- LateAircraftDelay: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airlines.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can interact with a DataFrame via SQLContext using SQL statements by registerting the DataFrame as a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "airlines.registerTempTable(\"airlines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*How many unique airlines are there?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|UniqueCarrier|\n",
      "+-------------+\n",
      "|           EA|\n",
      "|           UA|\n",
      "|           PI|\n",
      "|           PS|\n",
      "|           AA|\n",
      "|           NW|\n",
      "|           EV|\n",
      "|           B6|\n",
      "|           HP|\n",
      "|           TW|\n",
      "|           DL|\n",
      "|           OO|\n",
      "|           F9|\n",
      "|           YV|\n",
      "|           TZ|\n",
      "|           US|\n",
      "|           AQ|\n",
      "|           MQ|\n",
      "|           OH|\n",
      "|           HA|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uniqueAirline = sqlContext.sql(\"SELECT DISTINCT UniqueCarrier \\\n",
    "                                FROM airlines\")\n",
    "uniqueAirline.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Calculate how many flights completed by each carrier over time*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+\n",
      "|UniqueCarrier|FlightCount|\n",
      "+-------------+-----------+\n",
      "|           EA|     919785|\n",
      "|           UA|   13299817|\n",
      "|           PI|     873957|\n",
      "|           PS|      83617|\n",
      "|           AA|   14984647|\n",
      "|           NW|   10292627|\n",
      "|           EV|    1697172|\n",
      "|           B6|     811341|\n",
      "|           HP|    3636682|\n",
      "|           TW|    3757747|\n",
      "|           DL|   16547870|\n",
      "|           OO|    3090853|\n",
      "|           F9|     336958|\n",
      "|           YV|     854056|\n",
      "|           TZ|     208420|\n",
      "|           US|   14075530|\n",
      "|           AQ|     154381|\n",
      "|           MQ|    3954895|\n",
      "|           OH|    1464176|\n",
      "|           HA|     274265|\n",
      "+-------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 2 ms, sys: 0 ns, total: 2 ms\n",
      "Wall time: 1.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "carrierFlightCount = sqlContext.sql(\"SELECT UniqueCarrier, COUNT(UniqueCarrier) AS FlightCount \\\n",
    "                                    FROM airlines GROUP BY UniqueCarrier\")\n",
    "carrierFlightCount.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*How do you display full carrier names?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "carriers = sqlContext.read.format(\"com.databricks.spark.csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferschema\", \"true\")\\\n",
    "    .load(\"airlines/metadata/carriers.csv\")\\\n",
    "    .cache()\n",
    "carriers.registerTempTable(\"carriers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Code: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "carriers.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-----------+\n",
      "|         Description|UniqueCarrier|FlightCount|\n",
      "+--------------------+-------------+-----------+\n",
      "|Pinnacle Airlines...|           9E|     521059|\n",
      "|American Airlines...|           AA|   14984647|\n",
      "| Aloha Airlines Inc.|           AQ|     154381|\n",
      "|Alaska Airlines Inc.|           AS|    2878021|\n",
      "|     JetBlue Airways|           B6|     811341|\n",
      "|Continental Air L...|           CO|    8145788|\n",
      "|    Independence Air|           DH|     693047|\n",
      "|Delta Air Lines Inc.|           DL|   16547870|\n",
      "|Eastern Air Lines...|           EA|     919785|\n",
      "|Atlantic Southeas...|           EV|    1697172|\n",
      "|Frontier Airlines...|           F9|     336958|\n",
      "|AirTran Airways C...|           FL|    1265138|\n",
      "|Hawaiian Airlines...|           HA|     274265|\n",
      "|America West Airl...|           HP|    3636682|\n",
      "|Midway Airlines I...|       ML (1)|      70622|\n",
      "|American Eagle Ai...|           MQ|    3954895|\n",
      "|Northwest Airline...|           NW|   10292627|\n",
      "|         Comair Inc.|           OH|    1464176|\n",
      "|Skywest Airlines ...|           OO|    3090853|\n",
      "|Pan American Worl...|       PA (1)|     316167|\n",
      "+--------------------+-------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 9 ms, sys: 1 ms, total: 10 ms\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "carrierFlightCountFullName = sqlContext.sql(\"SELECT c.Description, a.UniqueCarrier, COUNT(a.UniqueCarrier) AS FlightCount \\\n",
    "                                    FROM airlines AS a \\\n",
    "                                    INNER JOIN carriers AS c \\\n",
    "                                    ON c.Code = a.UniqueCarrier \\\n",
    "                                    GROUP BY a.UniqueCarrier, c.Description \\\n",
    "                                    ORDER BY a.UniqueCarrier\")\n",
    "carrierFlightCountFullName.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What is the averaged departure delay time for each airline?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+---------------------------+-------------------+\n",
      "|first(Description, false)|first(UniqueCarrier, false)|        AvgDepDelay|\n",
      "+-------------------------+---------------------------+-------------------+\n",
      "|     Pinnacle Airlines...|                         9E| 7.9279144892173035|\n",
      "|     American Airlines...|                         AA|  7.862321254420546|\n",
      "|      Aloha Airlines Inc.|                         AQ| 1.5993176899118409|\n",
      "|     Alaska Airlines Inc.|                         AS|  8.297235193754096|\n",
      "|          JetBlue Airways|                         B6| 11.262714178314551|\n",
      "|     Continental Air L...|                         CO|  7.695967155526857|\n",
      "|         Independence Air|                         DH|  9.612639389688926|\n",
      "|     Delta Air Lines Inc.|                         DL|  7.593716274369933|\n",
      "|     Eastern Air Lines...|                         EA|  8.674050565435543|\n",
      "|     Atlantic Southeas...|                         EV| 13.483736343326541|\n",
      "|     Frontier Airlines...|                         F9|  6.096932123645889|\n",
      "|     AirTran Airways C...|                         FL|  10.27801937883596|\n",
      "|     Hawaiian Airlines...|                         HA|-0.5165400834606493|\n",
      "|     America West Airl...|                         HP|  8.107790266585615|\n",
      "|     Midway Airlines I...|                     ML (1)|  6.229676674364896|\n",
      "|     American Eagle Ai...|                         MQ|   9.22369994420141|\n",
      "|     Northwest Airline...|                         NW|  6.007973703240084|\n",
      "|              Comair Inc.|                         OH|  9.310795113723774|\n",
      "|     Skywest Airlines ...|                         OO|  7.193778047766392|\n",
      "|     Pan American Worl...|                     PA (1)|  5.532442442890681|\n",
      "+-------------------------+---------------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 999 ¬µs, sys: 2 ms, total: 3 ms\n",
      "Wall time: 6.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "avgDepartureDelay = sqlContext.sql(\"SELECT FIRST(c.Description), FIRST(a.UniqueCarrier), AVG(a.DepDelay) AS AvgDepDelay \\\n",
    "                                    FROM airlines AS a \\\n",
    "                                    INNER JOIN carriers AS c \\\n",
    "                                    ON c.Code = a.UniqueCarrier \\\n",
    "                                    GROUP BY a.UniqueCarrier \\\n",
    "                                    ORDER BY a.UniqueCarrier\")\n",
    "avgDepartureDelay.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "airlines.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Anaconda)",
   "language": "python",
   "name": "anaconda_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
